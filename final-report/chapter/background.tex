\chapter{The basics}

The fundamental question driving the study of computational complexity theory
is, ``how difficult are certain problems for computers to solve?''  In order to
answer this question precisely, we must start by figuring out what exactly it
asks.  That is, formally, what do we mean by \emph{difficulty}?  For that
matter, what constitutes a \emph{problem}?  What counts as a \emph{computer}?

Conventionally, \emph{computers} are formalized as Turing machines, with
\emph{difficulty} being measured by the number of Turing machine execution
steps.  For the purposes of this thesis, we avoid delving into the formalism of
Turing machines.  Instead, we assume an informal notion of computers given by
any algorithm or procedure straightforwardly implementable in modern,
high-level programming languages such as C/C++, Python, Java, etc.  Detailed
treatment of the relevant formalisms may be found in \textcite[Chapter
2]{papadimitriou.cc}.  In particular, there are theorems \parencite[Theorem
2.5]{papadimitriou.cc} showing that modern CPU/RAM-based computer architectures
are, for our purposes, equivalent to Turing machines, thereby justifying the
informal approach we take here.

In the following sections, we discuss what exactly constitutes a
\emph{problem}, how we describe the complexity (i.e., difficulty) of problems,
and how we categorize problems by difficulty into \emph{complexity classes}.

%emphasize intuitive descriptions of
%algorithms in terms of modern, ``high-level'' programming concepts exhibited by
%programming languages such as Python.

%An
%alternative treatment uses ``random access machines'', which mimic modern
%CPU/RAM-based computer architectures.  In this thesis, we avoid delving into
%these formal details.

%in terms of modern, high-level programming concepts

%In the conventional formalism, computers are modeled as Turing machines.
%Difficulty, then, refers to the number of execution steps required by a Turing
%Machine to solve a problem.  Alternatively, computers could be modeled as
%``random access machines'' \parencite[Section 2.6]{papadimitriou.cc}, which
%mimic modern CPU/RAM-based computer architectures.  For our purposes, the two
%models of computation are equivalent \parencite[Theorem 2.5]{papadimitriou.cc}.

%Formal treatments of these
%definitions are found in \textcite[Chapter 2]{papadimitriou.cc}

%Of particular
%note, \textcite[Theorem 2.5]{papadimitriou.cc} shows that these two models of
%computation are, for our purposes, equivalent.  Therefore, for our purposes, we will
%assume the latter model, allowing us to think in terms of modern programming
%patterns and give

%By \emph{hardness}, what we really mean is: given a problem input
%encoded in \(n\) bits, how much computational time, asymptotically with respect
%to \(n\), is required to solve that problem?  In order to discuss this question
%precisely, we have to clearly define what we mean by ``computational time'',
%and, for that matter, what we mean by ``computer''.  A traditional approach
%takes ``computer'' to mean Turing Machines and ``time'' to be Turing Machine
%execution steps; another approach defines ``computer'' via modern-day,
%CPU/RAM-based architectures, with ``time'' given by CPU instruction cycles.

%relatively \emph{informal} descriptions of algorithms.

\section{Decision problems}

The simplest flavor of computational problem is a \emph{decision problem}, or a
yes/no question: given an input \(X\), does \(X\) satisfy certain conditions?
Here are some examples of decision problems:
\begin{itemize}[nosep]
  \item Given an integer \(K\), is \(K\) even?
  \item Given a string of letters \(S\), is \(S\) a palindrome?
  \item A silly decision problem, but nevertheless a valid one: given any input
    \(X\), always return ``yes''.
\end{itemize}

In order for a yes/no question to qualify as a decision problem, it must be
stated in terms of an arbitrary input.  For instance, consider the following
question:
\begin{itemize}[nosep]
  \item Is \(314159\) a prime number?  (Answer: yes.  Proof: see WolframAlpha.)
\end{itemize}
This is a yes/no question, but it takes no inputs (the value \(314159\) is not
an input; it is merely part of the question statement).  In this sense, it is
computationally uninteresting: in order to solve this question, an algorithm
only needs to return the fixed answer ``yes''.  In contrast, what we're really
interested in is the general problem of primality testing:
\begin{itemize}[nosep]
  \item Given an arbitrary positive integer \(K\), is \(K\) prime?
\end{itemize}

We formalize the definition of decision problems below.

\begin{definition}{(decision) problem}{}

  A \Term{decision problem} is a function
  \(Œ†\colon\Set{0,1}^*‚Üí\Set{\text{yes},\text{no}}\).  Equivalently, a
  \Term{decision problem} is the set \(Œ†‚äÜ\Set{0,1}^*\) comprising exactly the
  inputs, a.k.a. \Term{instances}, that result in ``yes'' answers.

  That is, for any input \(X‚àà\Set{0,1}^*\), we say \(X‚ààŒ†\) (in the \emph{set}
  sense) if \(Œ†(X)=\text{yes}\) (in the \emph{function} sense), and \(X‚àâŒ†\) to
  mean \(Œ†(X)=\text{no}\).

  \begin{aside}
    Formally, inputs to decision problems are always encoded as binary strings.
    Essentially, this requirement follows from the fact that all modern
    computers encode data in binary anyway.  Furthermore, it allows us to
    rigorously discuss notions such as \emph{input size}.  This is an important
    formal detail, but for the most part, we avoid dealing with any binary
    encoding/decoding technicalities.  We mention this detail here only to
    clarify the role of \(\Set{0,1}^*\) in the definition above.
  \end{aside}

\end{definition}

There is another notion of \emph{problems}, called \Term{function problems}, as
arbitrary (binary-encoded) functions \(\Set{0,1}^*‚Üí\Set{0,1}^*\).  However,
function problems seem to generally receive less attention than decision
problems, perhaps because decision problems are conceptually simpler but still
versatile enough to capture the core ideas of complexity theory.  Whatever the
reason, this thesis abides by that tradition.  As such, the vast majority of
the problems examined in this thesis are decision problems, so for convenience
we will simply say ``problems'' to mean decision problems, unless otherwise
specified.

\section{Complexities and classes}

When we ask how difficult a problem is, we are essentially asking, how much
time (or other resources, such as memory) does a computer need to solve that
problem?  Of course, the answer depends on the input: some inputs are easy to
solve, and others are harder.  Certainly, we expect the difficulty to scale
with input size: the larger the input, the more work it generally takes an
algorithm to process it.  Thus, the complexity of a problem is given as a
function of the input size.  Specifically, we ask, if an algorithm is given an
input string (recall, encoded in binary) of length \(n\), how much time in the
worst case is required, as a function of \(n\)?

However, exact function bounds are unnecessarily sensitive to pedantic
technicalities, e.g., slight variations in implementations of the same
algorithm, or specific details in the formal models of ``computer''. Instead,
loosely speaking, we are mostly interested in how these costs asymptotically
\emph{scale} as the input size gets large.  Thus, we categorize problems with
``similar'' complexities into \emph{complexity classes}.

So then, what counts as \emph{similar}?  As a starting approximation, we assert
that \emph{polynomials are small}: any algorithm whose running time is bounded
by some polynomial function is considered relatively ``fast''; problems with
polynomial-time solutions are considered relatively ``easy''.  We formalize
this idea in the definition of the complexity class \P{} below.

\begin{definition}{Polynomial-time problems, \P}{}

  Let \(A\) be an algorithm computing some (decision) problem (i.e., it takes a
  binary string as input and returns ``yes'' or ``no'').  We say \(A\) runs in
  \Term{polynomial time} if there exists some polynomial \(p\) such that, on
  any input \(X‚àà\Set{0,1}^*\), the algorithm \(A\) terminates in \(‚â§p(\Abs X)\)
  steps.

  The complexity class \P{} is the set of (decision) problems correctly
  solvable in polynomial time.

  \begin{aside}
    For contrast, we say an algorithm is \Term{super-polynomial} if its running
    time cannot be bounded by some polynomial.  Examples of super-polynomial
    functions include \(n^{\log n}\), \(2‚Åø\), etc.
  \end{aside}

\end{definition}

To be clear, taking polynomials to mean ``easy'' is a very crude rule-of-thumb:
there are important practical subdivisions \emph{within} \P{} that this
categorization plainly ignores (e.g., linear-time vs quadratic-time); there are
also a few notable examples of super-polynomial-time algorithms that are, by
this rule, slow, but quite efficient \emph{in practice} (e.g., the simplex
algorithm for linear programming).  Nevertheless, this delineation remains an
extremely useful (and arguably elegant) starting point for the classification
of problems.

% TODO examples of ùêè problems, and perhaps discuss papadimitriou theorem 2.5
% again with more precision?

\section{Hard problems and reductions}

Above, we establish that a problem is considered \emph{easy} if it has a
polynomial-time solution.  Hard problems, then, are those without
polynomial-time solutions‚Ä¶ right?  Sure.  But how do we go about showing that a
problem is actually hard?  And how hard, exactly?

For an easy problem, proving \emph{existence} of a polynomial-time algorithm is
straightforward---simply construct one.  On the other hand, for a problem that
appears to be hard, we would have to prove \emph{non-existence} of a
polynomial-time algorithm---that it is \emph{impossible} to find a
polynomial-time algorithm. In general, this is incredibly difficult to show;
this difficulty is a large part of why the infamous \P-vs-\NP{} question
remains unsolved.

Thus, we take a different approach to understanding hard problems: by comparing
them to each other.

TODO: simple examples of two problems that reduce to each other, then
definition of reduction.  then definition of completeness.


Suppose we have two problems, \(Œ†‚ÇÅ\) and \(Œ†‚ÇÇ\).


%\section{Puzzles, non-determinism, and \NP}

%Consider the following problem.
%\begin{itemize}
%  \item Given a graph \(Œì\) and a positive integer \(k\), does \(Œì\) contain a
%    clique of size \(k\)?
%\end{itemize}


%\section{Hard problems and completeness}

%Notationally, we say \(X\) is \emph{in} the problem \(L\), or \(X\in L\), if
%the answer is yes; otherwise, we say \(X\notin L\).
%
%An example of a decision problem is the graph reachability problem:
%\begin{definition}[\Problem{reachability}]%
%  Given a graph with \(n\) vertices \(v_1, \dots, v_n\), does there exist a
%  path connecting \(v_1\) to \(v_n\)?
%\end{definition}
%This problem may be solved using simple graph-search algorithms such as
%Breadth-First/Depth-First Search, whose asymptotic running time is \(\O(n)\)
%---that is, bounded by a linear function of \(n\).  As such, this problem is
%considered relatively ``easy'' to solve.
%
%More generally, \Problem{reachability} belongs to the class of decision
%problems known as \P:
%\begin{definition}[\P]%
%  The class of decision problems whose solution runtime is bounded by a
%  polynomial function of the input length.
%\end{definition}
%We consider problems in \P{} to be ``easy''---at least, from the standpoint of
%computational complexity.
%
%Another example of a decision problem is the Hamiltonian path problem:
%\begin{definition}[\Problem{hamiltonian-path}]%
%  \label{def:hamiltonian-path} Given a graph with \(n\) vertices, does it
%  contain a Hamiltonian path (i.e., a path that visits each vertex exactly
%  once)?
%\end{definition}
%This problem is not known to be in \P.  In fact, the best known algorithms
%solving \Problem{hamiltonian-path} are essentially brute-force guess-and-check:
%\emph{guess} a possible Hamiltonian path (e.g., by writing down some
%permutation of the vertices), then \emph{check} that it is valid (e.g., that
%each pair of adjacent vertices in the guessed path are actually connected by an
%edge in the graph).  In the worst case, if our guesses are really unlucky, we
%may have to repeat up to \(n!\) iterations, which is definitely not polynomial.
%However, setting aside the cost associated with brute-forcing guesses, note
%that individual \emph{checking} steps \emph{do} run in polynomial time.
%Problems like this, which are solvable via guess-and-check, where the ``check''
%problem is in \P, belong to a class of problems known as \NP:
%\begin{definition}[\NP]%
%  \label{def:np} A decision problem \(L\) is in \NP{} if\dots
%  \begin{nested}
%    there exists a corresponding decision problem \(L'\in\P\) (intuitively: the
%    ``check'' problem) and a polynomial \(p\) such that\dots
%    \begin{nested}
%      for all input strings \(x\)\dots
%      \begin{nested}
%        \(x \in \NP\) if and only if\dots
%        \begin{nested}
%          there exists a ``guess'' \(g\) with length \(\Abs g \le p(\Abs x)\)
%          such that \((x, g) \in L'\) (intuitively: \(g\) passes the
%          ``check'').
%        \end{nested}
%      \end{nested}
%    \end{nested}
%  \end{nested}
%
%  Note that the \(\Abs g \le p(\Abs x)\) requirement is present in order to
%  ensure that the guesses are not so obscenely long as to abuse the idea of
%  ``efficient'' checking.  This requirement is not central to understanding the
%  definition of \NP{} but is nevertheless an important technical subtlety.
%\end{definition}
%
%The infamous \P-vs-\NP{} open question asks: is \NP{} truly more difficult than
%\P?  Does there exist some problem in \NP{} that definitively cannot be solved
%within polynomial time?  I, a baby undergraduate, am not in the business of
%answering that question.
%
%As such, the best we can do to determine the difficulty of a given problem is
%to compare them to other problems, deriving a \emph{relative} ordering telling
%us which problems are easier/harder than other ones.  To this end, we must
%define what easier/harder means---intuitively, we think of a problem \(L_1\) as
%easier than another problem \(L_2\) if knowing how to solve \(L_2\)
%automatically also tells us how to solve \(L_1\), with minimal
%(polynomially-bounded) overhead.  More precisely:
%\begin{definition}[reductions]
%  \label{def:reduction}
%  Let \(L_1\) and \(L_2\) be decision problems.  We say \(L_1\) is
%  \emph{reducible to} \(L_2\), or that \(L_1\) is \emph{at least as easy as}
%  \(L_2\)'', denoted \(L_1 \le L_2\), if\dots
%  \begin{nested}
%    there exists a function \(f\), called a \emph{reduction}, converting input
%    strings for \(L_1\) to inputs for \(L_2\), such that \(f\) is computable
%    within polynomial time, and\dots
%    \begin{nested}
%      for any input \(x_1\)\dots
%      \begin{nested}
%        \(x_1 \in L_1\) if and only if \(x_2 \in L_2\).
%      \end{nested}
%    \end{nested}
%  \end{nested}
%
%  Note that this definition of reductions is slightly different than the one
%  given in \textcite{papadimitriou.cc}, whose requirement on \(f\) is that it
%  is computable in \emph{logarithmic-space} rather than polynomial-time.
%  However, for the purposes of this project, the distinction between the two is
%  unimportant.
%\end{definition}
%
%This notion of comparison also gives us a good way of comparing problems to
%entire classes:
%\begin{definition}[hardness and completeness]%
%  \label{def:hard-complete}
%  Let \(\mathbfit C\) be a complexity class.
%  \begin{itemize}[nosep]
%    \item A problem \(L\) is \emph{hard for \(\mathbfit C\)}, or
%      \emph{\(\mathbfit C\)-hard}, if \(L\ge K\) for every \(K\in\mathbfit C\).
%    \item A problem \(L\) is \emph{complete for \(\mathbfit C\)}, or
%      \emph{\(\mathbfit C\)-complete}, if \(L\) is \(\mathbfit C\)-hard
%      \emph{and} \(L\in\mathbfit C\).
%  \end{itemize}
%\end{definition}
%
%In particular, \emph{complete} problems for a class \(\mathbfit C\) are at
%least as hard as everything else in \(\mathbfit C\) and simultaneously
%themselves \emph{in} \(\mathbfit C\).  In this sense, for any complexity class,
%its complete problems are its \emph{hardest} problems, giving us an effective,
%``exact'' characterization of the class in terms of its problems.
%
%This approach to characterizing complexity classes is the driving motivation
%behind our exploration of puzzles and games.
%
%%\todo[inline]{unfinished.  formalism of turing machines, decision problems,
%%  oracles \& the definition of polynomial hierarchy, proofs of completeness of
%%  SAT \& QSAT for classes in the polynomial hierarchy.  I imagine this stuff
%%  will be needed in the final thesis; is it needed also for the midyear
%%report?}
%%
%%\begin{definition}[decision problem/language]%
%%  A \textbf{decision problem} is a yes/no question posed on binary input
%%  strings, or problem \textbf{instances}.  As such, we may think of a decision
%%  problem as a mapping
%%  \[
%%    L \colon \Set{0, 1}^* \to \Set{\text{yes}, \text{no}}.
%%  \]
%%
%%  More commonly, we associate a problem with its ``yes'' instances, the set of
%%  which is a \textbf{language}:
%%  \[
%%    L(L) = \SetBuilder* {x \in \Set{0, 1}^*} {L(x) = \text{yes}}.
%%  \]
%%  Here, for clarity, we are distinguishing notationally between \(L\) and
%%  \(L(L)\), but in general we conflate the two notions and refer to both as
%%  the problem \(L\).
%%\end{definition}
%%
%%\begin{definition}[\NP]
%%  \NP{} is the class of problems solvable by a \emph{non-deterministic} Turing
%%  machine in \emph{polynomial time}.
%%\end{definition}
%%
%%
%%
%%
